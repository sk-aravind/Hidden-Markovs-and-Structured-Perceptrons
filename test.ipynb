{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-neutral': 1, 'I-neutral': 2, 'O': 3, 'B-positive': 4, 'I-positive': 5, 'B-negative': 6, 'I-negative': 7, 'stop': 8, 'start': 0}\n",
      "============================ CN ============================\n",
      "0  out of  546  tweets have been predicted.\n",
      "100  out of  546  tweets have been predicted.\n",
      "200  out of  546  tweets have been predicted.\n",
      "300  out of  546  tweets have been predicted.\n",
      "400  out of  546  tweets have been predicted.\n",
      "500  out of  546  tweets have been predicted.\n",
      "{'O': 1, 'B-INTJ': 2, 'B-PP': 3, 'B-NP': 4, 'I-NP': 5, 'B-VP': 6, 'B-PRT': 7, 'I-VP': 8, 'B-ADJP': 9, 'B-SBAR': 10, 'B-ADVP': 11, 'I-INTJ': 12, 'B-CONJP': 13, 'I-CONJP': 14, 'I-ADVP': 15, 'I-ADJP': 16, 'I-SBAR': 17, 'I-PP': 18, 'stop': 19, 'start': 0}\n",
      "============================ EN ============================\n",
      "0  out of  78  tweets have been predicted.\n",
      "{'O': 1, 'B-positive': 2, 'I-positive': 3, 'B-negative': 4, 'B-neutral': 5, 'I-negative': 6, 'I-neutral': 7, 'stop': 8, 'start': 0}\n",
      "============================ FR ============================\n",
      "0  out of  232  tweets have been predicted.\n",
      "100  out of  232  tweets have been predicted.\n",
      "200  out of  232  tweets have been predicted.\n",
      "{'O': 1, 'B-positive': 2, 'I-positive': 3, 'B-negative': 4, 'I-negative': 5, 'B-neutral': 6, 'I-neutral': 7, 'stop': 8, 'start': 0}\n",
      "============================ SG ============================\n",
      "0  out of  2698  tweets have been predicted.\n",
      "100  out of  2698  tweets have been predicted.\n",
      "200  out of  2698  tweets have been predicted.\n",
      "300  out of  2698  tweets have been predicted.\n",
      "400  out of  2698  tweets have been predicted.\n",
      "500  out of  2698  tweets have been predicted.\n",
      "600  out of  2698  tweets have been predicted.\n",
      "700  out of  2698  tweets have been predicted.\n",
      "800  out of  2698  tweets have been predicted.\n",
      "900  out of  2698  tweets have been predicted.\n",
      "1000  out of  2698  tweets have been predicted.\n",
      "1100  out of  2698  tweets have been predicted.\n",
      "1200  out of  2698  tweets have been predicted.\n",
      "1300  out of  2698  tweets have been predicted.\n",
      "1400  out of  2698  tweets have been predicted.\n",
      "1500  out of  2698  tweets have been predicted.\n",
      "1600  out of  2698  tweets have been predicted.\n",
      "1700  out of  2698  tweets have been predicted.\n",
      "1800  out of  2698  tweets have been predicted.\n",
      "1900  out of  2698  tweets have been predicted.\n",
      "2000  out of  2698  tweets have been predicted.\n",
      "2100  out of  2698  tweets have been predicted.\n",
      "2200  out of  2698  tweets have been predicted.\n",
      "2300  out of  2698  tweets have been predicted.\n",
      "2400  out of  2698  tweets have been predicted.\n",
      "2500  out of  2698  tweets have been predicted.\n",
      "2600  out of  2698  tweets have been predicted.\n",
      "============================ Predictions Complete ============================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "from part2 import * \n",
    "from preprocess import *\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "# function that collates a sorted list of different words, \n",
    "# and a dictionary of words as keys and associated indices as values \n",
    "def get_words(train):\n",
    "    # train: training data \n",
    "\n",
    "    all_words = [] # list of all words in train data\n",
    "\n",
    "    # populating all_words list\n",
    "    for tweet in range(len(train)):\n",
    "        for word in range(len(train[tweet])):\n",
    "            all_words.append(train[tweet][word][0]) # list of all words in data\n",
    "\n",
    "\n",
    "    diff_words = np.unique(all_words) # array of unique words\n",
    "\n",
    "    # generating dictionary of words as keys and associated indices as values \n",
    "    word_dict = defaultdict(int)\n",
    "    for i in range(len(diff_words)):\n",
    "        word_dict[diff_words[i]] = i\n",
    "\n",
    "    return diff_words, word_dict\n",
    "\n",
    "# function that gets different sentiments (tags) from a data set\n",
    "def get_tags(data):\n",
    "    \n",
    "    tags = {}\n",
    "    Y = get_counts(data)[0] \n",
    "    \n",
    "    len_tweet = 0\n",
    "    for sent in range(len(Y.keys())):\n",
    "        if (list(Y.keys())[sent] != 'start') and (list(Y.keys())[sent] != 'stop'):\n",
    "            tags[list(Y.keys())[sent]] = sent+1\n",
    "            len_tweet += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    tags['stop'] = len_tweet + 1\n",
    "    tags['start'] = 0\n",
    "    \n",
    "    return tags\n",
    "    \n",
    "    \n",
    "\n",
    "# function that converts emission dictionary into emission matrix\n",
    "def em_matrix(emissions, diff_words, sents={}):\n",
    "    # emissions: dictionary of emission probs\n",
    "    # diff_words: diff_words: array of words arranged with associated indices\n",
    "    # sents: dictionary of sentiments and associated indices\n",
    "\n",
    "    em_mat = np.zeros([len(sents.keys()), len(diff_words)]) # emission matrix\n",
    "    inv_sents = dict (zip(sents.values(),sents.keys())) # swapping keys and values\n",
    "\n",
    "    # populating emission matrix\n",
    "    for row in range(len(inv_sents.keys())):\n",
    "        for col in range(len(diff_words)):\n",
    "            if (diff_words[col], inv_sents[row]) in emissions.keys():\n",
    "                em_mat[row, col] = emissions[(diff_words[col], inv_sents[row])]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return em_mat\n",
    "\n",
    "\n",
    "# function that adds start and stop nodes to training set\n",
    "def mod_train (ptrain):\n",
    "    \n",
    "    train = copy.deepcopy(ptrain)\n",
    "    # inserting start and stop nodes\n",
    "    for tweet in train:\n",
    "        tweet.insert(0, ('////','start'))\n",
    "        tweet.append(('\\\\\\\\', 'stop'))\n",
    "        \n",
    "    return train\n",
    "\n",
    "\n",
    "# function that adds start and stop nodes to validation set\n",
    "def mod_test (ptest):\n",
    "    \n",
    "    test = copy.deepcopy(ptest)\n",
    "    # inserting start and stop nodes\n",
    "    for tweet in test:\n",
    "        tweet.insert(0, '////')\n",
    "        tweet.append('\\\\\\\\')\n",
    "        \n",
    "    return test\n",
    "\n",
    "\n",
    "# function that computes transition parameter matrix\n",
    "def transition_params(train, Y, sents):\n",
    "    # train: processed training set of features and labels\n",
    "    # Y: dictionary with sentiment and counts\n",
    "    # sents: dictionary with sentiments and associated indices\n",
    "\n",
    "    q_uv = np.zeros([len(Y.keys()), len(Y.keys())]) # 2D array transitions\n",
    "    \n",
    "    # counting u,v transitions for all u,v\n",
    "    for tweet in range(len(train)):\n",
    "        for y in range(1, len(train[tweet])):\n",
    "            \n",
    "            # comparing data labels with sentiment keys\n",
    "            label_i = sents[train[tweet][y][1]]\n",
    "            label_im1 = sents[train[tweet][y-1][1]]\n",
    "\n",
    "            # filling up transition matrix\n",
    "            q_uv[label_im1, label_i] += 1/Y[train[tweet][y-1][1]]\n",
    "\n",
    "    return q_uv\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "\n",
    "# function that runs the viterbi algorithm recursively \n",
    "def viterbi_algo (em_mat, trans_mat, \n",
    "                  word_dict, line, prev_scores, \n",
    "                  loop_ind=1, ind_list=[]):\n",
    "    # emissions: matrix of emission parameters\n",
    "    # transitions: matrix of transition parameters\n",
    "    # word_dict: dictionary of words with associated indices\n",
    "    # line: line of words (tweet)\n",
    "    # prev_col: scores of previous column \n",
    "    # loop_ind: current loop iteration\n",
    "\n",
    "    word_ind = word_dict[line[loop_ind][0]] # associated index of current word\n",
    "    emissions = em_mat[:, word_ind].reshape((len(trans_mat[0]),1)) \n",
    "    scores = emissions*trans_mat*np.transpose(prev_scores) # matrix of all scores \n",
    "    \n",
    "    current_scores = np.zeros([len(prev_scores), 1]) # init current word layer \n",
    "    # loop to fill current score column\n",
    "    for row in range(len(prev_scores)):   \n",
    "        current_scores[row, 0] = np.amax(scores[row,:])\n",
    "\n",
    "    # check statements to terminate recursion\n",
    "    if loop_ind < len(line)-1:\n",
    "        \n",
    "        loop_ind += 1 # setting next iterations index\n",
    "        ind_list.append(np.argmax(current_scores)) # storing optimal path node indices  \n",
    "        return viterbi_algo(em_mat, trans_mat, word_dict, line, current_scores, loop_ind, ind_list)\n",
    "    \n",
    "    else:\n",
    "        return ind_list\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# main code block\n",
    "if __name__ == '__main__':\n",
    "    outfile = '/dev.p3.out'\n",
    "    \n",
    "    # initializing dictionary where values are the nested list of optimal sentiments\n",
    "    opt_sent_dict = {'EN':[], 'CN':[], 'FR':[], 'SG:':[]} \n",
    "    \n",
    "    # loop over languages\n",
    "    for lang in languages:\n",
    "        \n",
    "        # reading tweets for particular language\n",
    "        ptrain = data_from_file(lang + '/train') # unmodified\n",
    "        train = mod_train (ptrain) # modified w/ start and stop states\n",
    "\n",
    "        # getting sentiments and associated indices (w/ start and stop)\n",
    "        sents = get_tags(ptrain) \n",
    "        print (sents)\n",
    "\n",
    "        Y = get_counts(train)[0] # dictionary of sentiments and their counts\n",
    "        diff_words = get_words(train)[0] # array of unique words \n",
    "        word_dict = get_words(train)[1] # dictionary of unique words and indices\n",
    "\n",
    "        # emission and transmission parameter matrices\n",
    "        emission_dict = get_emission(train) # dictionary with keys as (x, y) and values as emission probabilities\n",
    "        em_mat = em_matrix(emission_dict, diff_words, sents) # emission matrix\n",
    "        trans_mat = transition_params(train, Y, sents) # transition matrix\n",
    "        \n",
    "        # A list of list of tuples of size 1. Each list in test is a tweet. \n",
    "        ptest = data_from_file(lang + '/dev.in')\n",
    "        # test is a list of list. Each sublist is an array of words, 1 tweet\n",
    "        ptest = [[word[0] for word in line] for line in ptest]\n",
    "        test = mod_test(ptest) # modified with start and stop words\n",
    "        \n",
    "        # initializing list of optimal sentiment lists corresponding to each tweet \n",
    "        optimal_sentiments = []\n",
    "        \n",
    "        print ('============================',lang , '============================')\n",
    "        # loop that runs over all tweets for a given language to predict optimal sentiments\n",
    "        for tweet in range(len(test)):\n",
    "            \n",
    "            base_scores = np.ones([len(sents.keys()),1]) # initializing base case scores\n",
    "            opt_ind_list = viterbi_algo (em_mat, trans_mat, word_dict, test[tweet], base_scores, 1, []) # running Viterbi\n",
    "            \n",
    "            # generating list of optimal sentiments for a given sentence\n",
    "            inv_sents = dict (zip(sents.values(), sents.keys())) # swapping keys and values\n",
    "            opt_sents = [inv_sents[opt_ind_list[i]] for i in range(len(opt_ind_list))]\n",
    "\n",
    "            optimal_sentiments.append(opt_sents) # populating parent optimal sentiment list\n",
    "            \n",
    "            # printing iteration checks\n",
    "            if (tweet % 100 == 0):\n",
    "                print (tweet, ' out of ', len(test), ' tweets have been predicted.')\n",
    "        \n",
    "        opt_sent_dict[lang] = optimal_sentiments # populating optimal sentiment dictionary \n",
    "        \n",
    "        predictions = []\n",
    "        for tweet in range(len(optimal_sentiments)):\n",
    "            predictions.append([(ptest[tweet][i], optimal_sentiments[tweet][i]) for i in range(len(optimal_sentiments[tweet]))])\n",
    "        \n",
    "        write_predictions(predictions, lang, outfile)\n",
    "        \n",
    "    \n",
    "    print ('============================ Predictions Complete ============================')\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "# lang = 'SG'\n",
    "# ptest = data_from_file(lang + '/dev.in')\n",
    "# ptest = [[word[0] for word in line] for line in ptest]\n",
    "# test = mod_test(ptest)\n",
    "\n",
    "# test_ind = 0\n",
    "\n",
    "# print ('length of tweet: ', len(test[test_ind]))\n",
    "# print ('tweet: ', test[test_ind])\n",
    "# print ('tweet: ', ptest[test_ind])\n",
    "# print ()\n",
    "\n",
    "# print ('length of tweet: ', len(opt_sent_dict[lang][test_ind]))\n",
    "# print ('optimal sentiments: ', opt_sent_dict[lang][test_ind])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
