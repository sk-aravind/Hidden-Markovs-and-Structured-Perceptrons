# function that collates a sorted list of different words, 
# and a dictionary of words as keys and associated indices as values 
def get_words (train):
    # train: training data 

    all_words = [] # list of all words in train data
    
    # populating all_words list
    for tweet in train:
        for y in tweet:
            all_words.append(y[0]) # list of all words in data

    diff_words = np.unique(all_words).tolist() # list of unique words
    diff_words.append('#UNK#') # appending #UNK#
    
    # generating dictionary of words as keys and associated indices as values 
    word_dict = defaultdict(int)
    for i in range(len(diff_words)):
        word_dict[diff_words[i]] = i

    return np.asarray(diff_words), word_dict



# function that gets different tags (sentiments) from a data set
def get_tags (data):
    
    tags = defaultdict(int)
    Y = get_count_y(data) 
    
    len_tweet = 0
    for sent in Y:
        if (Y[sent] != 'start') and (Y[sent] != 'stop'):
            len_tweet += 1
            tags[sent] = len_tweet
        else: pass
    
    tags['stop'] = len_tweet + 1
    tags['start'] = 0
    
    return tags



# function that adds start and stop nodes to validation set
def mod_test (ptest):
    
    test = copy.deepcopy(ptest)
    # inserting start and stop nodes
    for tweet in test:
        tweet.insert(0, '~~~~|')
        tweet.append('|~~~~')
        
    return test


# function that runs the viterbi algorithm for each tweet
    # a: transition dictionary
    # b: emission dictionary
    # tags: dictionary of tags and indices
    # words: dictionary of words
    # tweet: tweet from data
def Viterbi (a, b, tags, words, tweet):
    
    optimal_tags = [] # optimal tags for given tweet
    
    pi = defaultdict(float) # initializing score dictionary
    pi[(0, 'start')] = 1. # base case
    
    for j in range(1,len(tweet)): # loop over all words in tweet
        
        u_opt, pi_j_max = ['O', 0.] # default tag and score
        x_j = tweet[j] if tweet[j] in words else '#UNK#' # j-th word in tweet
        
        for u in tags: # loop over all possible tags
            
            pi[(j, u)] = max([pi[(j-1, v)] * a[(v,u)] * b[(x_j, u)] for v in tags]) # max score finding
            u_opt, pi_j_max = [u, pi[(j, u)]] \
                                if pi[(j, u)] > pi_j_max \
                                else [u_opt, pi_j_max] # updating opt tag for x_j
            
        optimal_tags.append(u_opt) # appending optimal sentiments
        
    return optimal_tags[:-1]






# main code block
outfile = '/dev.p3.out'

# loop over languages
for lang in languages:

    print ('============================',lang , '============================')
    # ====================================== training ======================================
    k = 3
    a, b, tags, words = train_phase (lang, k)
    # ======================================================================================
    
    # ========================================= validation set =========================================
    # A list of list of tuples of size 1. Each list in test is a tweet. 
    ptest = data_from_file(lang + '/dev.in')
    # test is a list of list. Each sublist is an array of words, 1 tweet
    ptest = [[word[0] for word in line] for line in ptest]
    test = mod_test(ptest) # modified with start and stop words
    # ==================================================================================================

    start = time.time()
    # ============================================ getting predictions ============================================
    # initializing list of optimal sentiment lists corresponding to each tweet 
    optimal_sentiments = []

    # loop that runs over all tweets for a given language to predict optimal sentiments
     for tweet in test:
         optimal_sentiments.append(Viterbi (a, b, tags, words, tweet))


    predictions = []

     for tweet in range(len(optimal_sentiments)):
         predictions.append([(ptest[tweet][i], optimal_sentiments[tweet][i]) for i in range(len(optimal_sentiments[tweet]))])

    write_predictions(predictions, lang, outfile) # writing predictions to outfile
    # =============================================================================================================
    end = time.time()

    print('time to get predictions for', lang, ': ', end - start)
    print ()
    pred = get_entities(open(lang+outfile, encoding='utf-8'))
    gold = get_entities(open(lang+'/dev.out', encoding='utf-8'))
    print (lang)
    compare_result(gold, pred)
    print ()

print ('============================ Predictions Complete ============================')