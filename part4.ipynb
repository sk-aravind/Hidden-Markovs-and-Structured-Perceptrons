{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================ EN ============================\n",
      "defaultdict(<class 'int'>, {'O': 2, 'B-INTJ': 3, 'B-PP': 4, 'B-NP': 5, 'I-NP': 6, 'B-VP': 7, 'B-PRT': 8, 'I-VP': 9, 'B-ADJP': 10, 'B-SBAR': 11, 'B-ADVP': 12, 'I-INTJ': 13, 'B-CONJP': 14, 'I-CONJP': 15, 'I-ADVP': 16, 'I-ADJP': 17, 'I-SBAR': 18, 'I-PP': 19, 'stop0': 20, 'stop1': 21, 'start0': 0, 'start1': 1})\n",
      "There are 153 columns out of  484 with probabilities > 1e-15\n",
      "0  out of  78  tweets have been predicted.\n",
      "============================ FR ============================\n",
      "defaultdict(<class 'int'>, {'O': 2, 'B-positive': 3, 'I-positive': 4, 'B-negative': 5, 'B-neutral': 6, 'I-negative': 7, 'I-neutral': 8, 'stop0': 9, 'stop1': 10, 'start0': 0, 'start1': 1})\n",
      "There are 27 columns out of  121 with probabilities > 1e-15\n",
      "0  out of  232  tweets have been predicted.\n",
      "100  out of  232  tweets have been predicted.\n",
      "200  out of  232  tweets have been predicted.\n",
      "============================ Predictions Complete ============================\n"
     ]
    }
   ],
   "source": [
    "# part 4 of ML project\n",
    "from part4_fun import *\n",
    "from check_fun import *\n",
    "\n",
    "\n",
    "# main code block\n",
    "if __name__ == '__main__':\n",
    "    outfile = '/dev.p4.out'\n",
    "    \n",
    "    # loop over EN and FR languages\n",
    "    for lang in ['EN', 'FR']:\n",
    "        \n",
    "        print ('============================',lang , '============================')\n",
    "        \n",
    "        # ======================================== training ========================================\n",
    "        # reading tweets for particular language\n",
    "        ptrain = data_from_file(lang + '/train') # unmodified\n",
    "        train = mod_train2 (ptrain) # modified w/ start and stop states\n",
    "\n",
    "        # getting sentiments/sentiment pairs and associated indices (w/ start and stop)\n",
    "        sents = get_tags2(ptrain) \n",
    "        sent_pairs = get_tags2pairs(sents)\n",
    "        print (sents)\n",
    "\n",
    "        YY = get_counts2(train, sent_pairs) # dictionary of sentiment pairs and their counts\n",
    "        diff_words = get_words(train)[0] # array of unique words \n",
    "        word_dict = get_words(train)[1] # dictionary of unique words and indices\n",
    "\n",
    "        # emission and transmission parameter matrices\n",
    "        emission_dict = get_emission2(train, 1) # dictionary with keys as (x, y) and values as emission probabilities\n",
    "        em_mat = em_matrix(emission_dict, diff_words, sents) # emission matrix\n",
    "        trans_mat2 = transition_params2(train, YY, sents, sent_pairs) # transition matrix\n",
    "        trans_mat2 = mod_trans2(trans_mat2) # modifying transition matrix \n",
    "        # ========================================================================================== \n",
    "        non_zero_trans = transition_check(trans_mat2, 1e-15)\n",
    "        \n",
    "        \n",
    "        # ==================================== validation set ====================================\n",
    "        # A list of list of tuples of size 1. Each list in test is a tweet. \n",
    "        ptest = data_from_file(lang + '/dev.in')\n",
    "        # test is a list of list. Each sublist is an array of words, 1 tweet\n",
    "        ptest = [[word[0] for word in line] for line in ptest]\n",
    "        test = mod_test2(ptest) # modified with start and stop words\n",
    "        # ========================================================================================\n",
    "        \n",
    "        \n",
    "        # ==================================== getting predictions ====================================\n",
    "        # initializing list of optimal sentiment lists corresponding to each tweet \n",
    "        optimal_sentiments = []\n",
    "        \n",
    "        # loop that runs over all tweets for a given language to predict optimal sentiments\n",
    "        for tweet in range(len(test)):\n",
    "            \n",
    "            # running Viterbi algorithm\n",
    "            base_scores = np.ones([len(sents.keys()),1]) # initializing base case scores\n",
    "            opt_ind_list = viterbi_algo2(em_mat, trans_mat2, word_dict, test[tweet], base_scores, base_scores, 2, []) \n",
    "            \n",
    "            # generating list of optimal sentiments for a given sentence\n",
    "            inv_sents = dict (zip(sents.values(), sents.keys())) # swapping keys and values\n",
    "            opt_sents = [inv_sents[opt_ind_list[i]] for i in range(len(opt_ind_list))]\n",
    "            \n",
    "            # populating parent optimal sentiment list\n",
    "            optimal_sentiments.append(opt_sents) \n",
    "            \n",
    "            # printing iteration checks\n",
    "            if (tweet % 100 == 0):\n",
    "                print (tweet, ' out of ', len(test), ' tweets have been predicted.')\n",
    "        \n",
    "        predictions = [] # init list of predictions\n",
    "        for tweet in range(len(optimal_sentiments)):\n",
    "            predictions.append([(ptest[tweet][i], optimal_sentiments[tweet][i]) for i in range(len(optimal_sentiments[tweet]))])\n",
    "        \n",
    "        write_predictions(predictions, lang, outfile) # writing results to outfile\n",
    "        # =============================================================================================\n",
    "        \n",
    "    \n",
    "    print ('============================ Predictions Complete ============================')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
